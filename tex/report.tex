\documentclass[12pt]{article}

\usepackage{mystyle}
\usepackage{fancyvrb}
\usepackage{tikz}
\hypersetup{
    colorlinks=true,
    urlcolor=blue,
    }

\renewcommand{\algorithmicrequire}{\textbf{Inputs:}}
%\renewcommand{\algorithmiccomment}[1]{// #1}
\newcommand{\nb}{b}
\date{}
\VerbatimFootnotes

\title{Optimizing Computing the $Q$ Factor in LAPACK}
\author{Johnathan Rhyne\\ Advised by: Julien Langou}


% Add talking about org2r against orgqr 


\begin{document}
    \maketitle
    \begin{abstract}
    Computing the Q factor from a sequence of Householder elementary reflections is an important operation for some 
    applications. We present new algorithms which speed up the performance of these operations. In addition, we also 
    implement existing algorithms originally developed by Puglisi~\cite{Puglisi} and Joffrain et al.~\cite{Joff} to speed up the computation of the T matrix. We see 
    performance improvements on the order of 2 to 5 times more efficiency in comparison to the reference LAPACK 
    computations and see similar performance in many cases as the optimized implementation for AMD (AOCL). Using these 
    schemes we not only see an improvement in execution time, but we lower the memory footprint for the main driver 
    algorithm.
    \end{abstract}


    \section{Preliminaries}
    A Householder elementary reflection is of the form
    \begin{equation}\label{eq:elementary}
        \left(I - \tau vv^\top\right)
    \end{equation}
    where, in real arithmetic, $\tau$ is $2/\|v\|_2^2$. $v$ can be any nonzero vector.
    These reflections are used to introduce ``zeros'' in matrices. And typically several of these matrices are applied, one-at-a-time, 
    to a matrix so that the product of these Householder elementary reflections represent the orthogonal matrix which is applied to
    the initial matrix either from the left or from the right. It is important for some applications to explicitly compute this orthogonal matrix and, in this report, we are interested in
    this computation process. In a QR factorization, this orthogonal matrix would be the ``Q factor'' of the QR factorization. This is the 
    reason why we say ``the Q factor''. The Q factor is computed by LAPACK routines \verb|DORG2R| and \verb|DORGQR|.

    To compute the Q factor, an orthogonal matrix $Q_m\in\R^{m\times m}$, we apply our Householder reflectors to the identity matrix to form:
    \begin{equation}\label{eq:Q}
        Q_m = \left(I - \tau_1 v_1v_1^\top\right)\cdots\left(I - \tau_kv_kv_k^\top\right)
    \end{equation}

    We will assume that the matrix $V$ containing the vectors $v_1$, ..., $v_k$ is unit lower triangular. Which is the case in our applications.
    Because of this structure, it is more advantageous to apply from the right than from the left.
    In addition, since we will apply from the right, 
    we note that, if we don't want all $m$ of the columns, we can instead choose a smaller amount denoted $n$ 
    and this lets us get $Q_n\in\R^{m\times n}$ by 
    \begin{equation}\label{eq:Qn}
        Q_n = \left(I - \tau_1 v_1v_1^\top\right)\cdots\left(I - \tau_kv_kv_k^\top\right)I_{m\times n}
    \end{equation}
    Where the $i,j$th element of $I_{m\times n}$ is given by:
    \begin{equation*}
        I_{m\times n}(i,j) = \begin{cases}
            1 & 1\leq i = j\leq n,\\
            0 & \text{otherwise}.
        \end{cases}
    \end{equation*}
    Throughout this discussion, we will omit the subscripts, and instead we will be assuming the size to be 
    what makes sense in context. In general, algorithms will compute $Q_n$ while our theory will discuss computing $Q_m$. 

    We will always have that $$m\geq n\geq k.$$

    We are mostly interested in the following two cases
    \begin{enumerate}
        \item $m=n\geq k$
        \item $m\geq n=k$.
    \end{enumerate}
    In other words, either $n$ is $m$ or $n$ is $k$. The case $m>n>k$ is made possible because it is easily handled, but it does not seem
    that it has many applications.

    \section{DORGQR Overview}

    \subsection{Motivation}

    One method to compute $Q$ is given by Algorithm~\ref{alg:dorg2r}

    \begin{algorithm}
        \caption{Basic algorithm to compute $Q$}\label{alg:dorg2r}
        \begin{algorithmic}[1]
            \STATE $Q= I_{m\times n}$
            \FOR{$i=k,\dots,1$}
                \STATE $Q= \left(I-\tau_iv_iv_i^\top\right)Q$
            \ENDFOR
        \end{algorithmic}
    \end{algorithm}

    A straightforward implementation of Algorithm~\ref{alg:dorg2r} will do so via matrix-vector products. While matrix-vector products
    are heavily optimized, they are intrinsically memory bound, and we can be more efficient by instead trying to use matrix-matrix
    operations. We will do so by using the compact WY representation of a product of Householder reflections~\cite{doi:10.1137/0910005}.
    Given $\nb$ reflections, 
    the compact WY representation constructs a matrix $T$ such that 
    \[
        I - VTV^\top = \left(I - \tau_1 v_1v_1^\top\right)\cdots\left(I - \tau_{\nb}v_{\nb}v_{\nb}^\top\right).
    \]
    The $I - VTV^\top$ representation enables us to apply the $\nb$ reflections all at once and relies on matrix-matrix multiplications, 
    as opposed to apply the reflections one at a time and relying on matrix-vector products with $\left(I - \tau_1 v_1v_1^\top\right)\cdots\left(I - \tau_{\nb}v_{\nb}v_{\nb}^\top\right)$.

    Given a set of $\nb$ vectors $V$ and associated $\tau$'s, the LAPACK routine \verb|DLARFT| constructs $T$.
    Given a set of $\nb$ vectors $V$ and associated $T$, the LAPACK routine \verb|DLARFB| applies $I - VTV^\top$ to a matrix.

%   In order to explain the compact WY representation, we first look at the case of $k=2$.
%
%   When this happens, we see that Equation~(\ref{eq:Q}) gives us the following
%   \begin{align*}
%       Q &= \left(I-\tau_1v_1v_1^\top\right)\left(I-\tau_2v_2v_2^\top\right) \\
%       &= I - \tau_1v_1v_1^\top - \tau_2v_2v_2^\top + \tau_1\tau_2v_1v_1^\top v_2v_2^\top
%   \end{align*}
%   Defining the matrices
%   \begin{align*}
%       V &= \begin{bmatrix} v_1 & v_2 \end{bmatrix} \\
%       T &= \begin{bmatrix}
%           \tau_1 & \tau_1\tau_2v_1^\top v_2 \\
%           0      & \tau_2
%       \end{bmatrix}
%   \end{align*}
%   We can rewrite above as 
%   \[
%       Q = I - VTV^\top
%   \]
%   In fact, we can extend this even further to the case where we assume we already collected some of our reflectors and we now want to collect these ``blocks''
%   \begin{align*}
%       Q &= \left(I - V_1T_{1,1}V_1^\top\right)\left(I - V_2T_{2,2}V_2^\top\right) \\
%       &= I - V_1T_{1,1}V_1^\top - V_2T_{2,2}V_2^\top + V_1T_{1,1}V_1^\top V_2T_{2,2}V_2^\top \\
%       &= I - V_1T_{1,1}V_1^\top - V_2T_{2,2}V_2^\top + V_1\left(T_{1,1}V_1^\top V_2T_{2,2}\right)V_2^\top 
%   \end{align*}
%   Similarly as above, we define the following matrices
%
%   \begin{align*}
%       V &= \begin{bmatrix} V_1 V_2 \end{bmatrix}\\
%       T &= \begin{bmatrix} 
%           T_{1,1} & T_{1,2} \\
%           0       & T_{2,2}
%       \end{bmatrix}
%   \end{align*}
%   Where 
%   $$
%   T_{1,2} = T_{1,1}V_1^\top V_2T_{2,2}.
%   $$
%   We see that 
%   \[
%       Q = I - VTV^\top
%   \]
%
%   Given a $V$ and associated $\tau$, the LAPACK routine that construct $T$ is \verb|DLARFT|.



    \subsection{Existing Behavior}

    The way that $Q$ is computed in LAPACK is via Algorithm~\ref{alg:dorgqr}.

    \begin{algorithm}
        \caption{Blocked computation of $Q$}\label{alg:dorgqr}
        \begin{algorithmic}[1]
            \STATE Determine blocking parameter $\nb$
            \STATE $Q = I_{m\times n}$
            \FOR{Each block of size at most $\nb$ of $V$ moving right to left}
                \STATE (\verb|DLARFT|) Compute $T$
                \STATE (\verb|DLARFB|) Apply $I-V_{\nb}TV_{\nb}^\top$ to the trailing columns of $Q$
                \STATE (\verb|DORG2R|) Apply $I-V_{\nb}TV_{\nb}^\top$ to itself
            \ENDFOR
        \end{algorithmic}
    \end{algorithm}

    Note: $V_{\nb}$ is the current collection of $\nb$ reflectors.

    The parameter $\nb$ should be tuned for enabling good performance. The default value for $\nb$ in the reference LAPACK library is $32$.
%\footnote{To determine your own machine's block size for a given routine, see the interface for 
%\verb+ilaenv+ and call this function}. 
To determine your own machine's block size for a given routine, see the interface for 
    \verb+ilaenv+ and call this function.
We don't investigate this choice much in this report, but we offer 
    some pointers to how investigating this choice could be beneficial.

    \subsection{Places for Optimization}
    Looking at Algorithm~\ref{alg:dorgqr}, we see four main opportunities for efficiency improvements:
    \begin{enumerate}
        \item Improving the algorithm used inside \verb|DLARFT|
        \item Creating a specialized version of \verb|DLARBF|
        \item Rewriting the first iteration to exploit the fact that $Q$ was initialized as the identity.
        \item Replacing the call to \verb|DORG2R| with a call to a specialized block application to exploit $T$
    \end{enumerate}

    \section{Computation of T}\label{sec:larft}
    Before we discuss the improvement of \verb|DLARFT|, we first motivate the current behavior which we will
    extend.
    
    Looking back at Equation~($\ref{eq:Q}$), we first consider the case of $k=2$, and see:
    \begin{align*}
        Q_m &= \left(I - \tau_1v_1v_1^\top\right)\left(I - \tau_2v_2v_2^\top\right) \\
            &= I - \tau_1v_1v_1^\top - \tau_2v_2v_2^\top + \tau_1v_1v_1^\top\tau_2v_2v_2^\top \\
            &= I - \tau_1v_1v_1^\top - \tau_2v_2v_2^\top + \left(\tau_1\tau_2v_1^\top v_2\right)v_1v_2^\top
    \end{align*}
    If we now define the matrices
    \begin{align*}
        V &= \begin{bmatrix} v_1 & v_2 \end{bmatrix} \\
            T &= \begin{bmatrix} \tau_1 & \tau_1\tau_2v_1^\top v_2 \\
            0 & \tau_2\end{bmatrix}.
    \end{align*}
    Then, we can rewrite Equation~(\ref{eq:Q}) as 
    \begin{equation}\label{eq:blockedQ}
        Q_m = I - VTV^\top.
    \end{equation}

    Note: $V$ is unit lower triangular and $T$ is upper triangular.

    Naturally, we can extend this kind of analysis to an arbitrarily large $k$. In this case, we want to
    instead assume that we already did above and collected some of our reflectors on ``both sides''. This means
    we can rewrite equation \ref{eq:Q} as
    \begin{align*}
        Q_m &= \left(I-\tau_1v_1v_1^\top\right)\cdots\left(I-\tau_kv_kv_k^\top\right) \\
            &= \left(I - V_1T_{1,1}V_1^\top\right)\left(I - V_2T_{2,2}V_2^\top\right) \\
            &= I - V_1T_{1,1}V_1^\top - V_2T_{2,2}V_2^\top + V_1T_{1,1}V_1^\top V_2T_{2,2}V_2^\top \\
            &= I - V_1T_{1,1}V_1^\top - V_2T_{2,2}V_2^\top + V_1\left(T_{1,1}V_1^\top V_2T_{2,2}\right)V_2^\top
    \end{align*}
    Which then lets us recover Equation~(\ref{eq:Q}) where
    \begin{align}
        V &= \begin{bmatrix} V_1 & V_2 \end{bmatrix} \label{eq:vMat} \\
        T &= \begin{bmatrix} T_{1,1} & T_{1,2} \\
        0 & T_{2,2}\end{bmatrix} \label{eq:tMat}
    \end{align}
    where $T_{1,2}$ is given by Equation~(\ref{eq:T12}).
    \begin{equation}\label{eq:T12}
        T_{1,2} = T_{1,1}V_1^\top V_2T_{2,2}
    \end{equation}


    \subsection{Existing Behavior}
    The current implementation of \verb|DLARFT| (as of LAPACK version 3.12) computes $T$ via Equation~(\ref{eq:tMat}) and Equation~(\ref{eq:T12}) but with either $T_{1,1}$ or $T_{2,2}$ of size $1\times 1$ (depending on the direction called). We will present the behavior for the case when $T_{1,1}\in\R^{1\times 1}$ in Algorithm~\ref{alg:refDLARFT}.

    \begin{algorithm}
        \caption{Reference DLARFT}\label{alg:refDLARFT}
        \begin{algorithmic}[1]
            \REQUIRE $V\in\R^{m\times n}, T\in\R^{n\times n}, \tau\in\R^{n}$ \hfill\COMMENT{$m \geq n$}
            \STATE $T(1,1) = \tau_1$
            \FOR{$i = 2, \dots, n$}
                \STATE $T(1:i-1,i) = -\tau_iV(:,1:i-1)^\top V(:,i)$
                \STATE $T(1:i-1,i) = T(1:i-1,1:i-1)T(1:i-1,i)$
                \STATE $T(i,i) = \tau_i$
            \ENDFOR
        \end{algorithmic}
    \end{algorithm}
    This algorithm accomplishes what needs to be done, but is unfortunately based on matrix-vector products.
    While these operations are optimized, we can take advantage of our processor better if we can rewrite this
    to instead use matrix-matrix operations.
    \subsection{Recursive LARFT}
    To try to take advantage of our matrix-matrix operations, we revisit the characterization of $V$ and $T$
    given by equations \ref{eq:vMat} and \ref{eq:tMat} but instead of forcing $T_{1,1}$ to be of size $1$, we 
    allow it be larger. This leaves us with a recursive version given by \ref{alg:recDLARFT}
    
    \begin{algorithm}
        \caption{Recursive DLARFT}\label{alg:recDLARFT}
        \begin{algorithmic}[1]
            \REQUIRE $V\in\R^{m\times n}, T\in\R^{n\times n}, \tau\in\R^n$\hfill\COMMENT{$m\geq n\geq 1$}
            \IF{ $n = 1$ }
                \STATE $T(1,1) = \tau_1$
                \RETURN
            \ENDIF
            \STATE $k = \lceil\frac{n}{2}\rceil$%\hfill\COMMENT{Round down if needed}
            \STATE $V_1 = V(:,1:k)$
            \STATE $V_2 = V(:,k+1:n)$
            \STATE Compute $T_{1,1}$ by calling this routine with $V_1$, $T(1:k,1:k)$, and $\tau(1:k)$
            \STATE Compute $T_{2,2}$ by calling this routine with $V_2$, $T(k+1:n,k+1:n)$, and $\tau(k+1:n)$
            \STATE $T_{1,2} = V_1^\top V_2$
            \STATE $T_{1,2} = -T_{1,1}T_{1,2}$
            \STATE $T_{1,2} = T_{1,2}T_{2,2}$
            \RETURN
        \end{algorithmic}
    \end{algorithm}

    A visual representation of what this algorithm does can be seen in Figure~\ref{fig:recCall} and Figure~\ref{fig:termCase}.

    \begin{figure}
        \begin{center}
        \begin{tikzpicture}
            \draw  (4,4) % Top right
                -- (4,0) % bottom right
                -- (0,4) % top left
                -- cycle;
            \draw  (2,4)
                -- (2,2)
                -- (4,2);
            \node at (1.333,3.333) {$T_{1,1}$};
            \node at (3,3) {$T_{1,2}$};
            \node at (3.333,1.333) {$T_{2,2}$};
            \draw[->] (5,2) -- (6,2);
            \draw (7, 4) -- (11,4) -- (11, 0) -- cycle;
            \draw[fill=blue] (7, 4) -- (7.75, 4) -- (7.75, 3.25) -- cycle;
            \draw[fill=orange]  (7.75,4) -- (8.5,4) -- (8.5,3.25) -- (7.75,3.25) -- cycle;
            \draw[fill=blue] (7.75, 3.25) -- (8.5, 3.25) -- (8.5, 2.5) -- cycle;
            %\draw[fill=orange]   -- (10,4) -- (10,2.5) --  -- cycle;
            \draw[fill=blue] (8.5,2.5) -- (10, 2.5) -- (10, 1) -- cycle;
            \draw[fill=orange] (10,2.5) -- (11,2.5) -- (11,1) -- (10,1) -- cycle;
            \draw[fill=blue] (10,1) -- (11,1) -- (11,0) -- cycle;
            \draw[fill=green!75!black] (8.5,4) -- (11,4) -- (11,2.5) -- (8.5,2.5) -- cycle;
        \end{tikzpicture}
    \end{center}
        \caption{Visual representation of the $2^\text{nd}$ layer in recursively computing $T$. The blue triangles
        work together to compute the orange portions. Then the triangles made by combining the blue and orange
        pieces finally compute the green piece}\label{fig:recCall}.
    \end{figure}
    \begin{figure}
        \begin{center}
        \begin{tikzpicture}
            \draw  (4,4) % Top right
                -- (4,0) % bottom right
                -- (0,4) % top left
                -- cycle;
            \draw  (2,4)
                -- (2,2)
                -- (4,2);
            \node at (1.333,3.333) {$T_{1,1}$};
            \node at (3,3) {$T_{1,2}$};
            \node at (3.333,1.333) {$T_{2,2}$};
            \draw[->] (5,2) -- (6,2);
            \draw (7, 4) -- (11,4) -- (11, 0) -- cycle;
            \draw[fill=blue] (7,4) -- (7.5,4) -- (7.5,3.5) -- cycle;
            \draw[fill=blue] (7.5,3.5) -- (8.0, 3.5) -- (8.0, 3.0) -- cycle;
            \draw[fill=blue] (8.0,3.0) -- (8.5, 3.0) -- (8.5, 2.5) -- cycle;
            \draw[fill=blue] (8.5,2.5) -- (9, 2.5) -- (9, 2) -- cycle;
            \draw[fill=blue] (9,2) -- (9.5, 2) -- (9.5, 1.5) -- cycle;
            \draw[fill=blue] (9.5,1.5) -- (10, 1.5) -- (10, 1) -- cycle;
            \draw[fill=blue] (10,1) -- (10.5, 1) -- (10.5, .5) -- cycle;
            \draw[fill=blue] (10.5, .5) -- (11, .5) -- (11, 0) -- cycle;
        \end{tikzpicture}
        \end{center}
        \caption{Visual representation of the terminating case in recursively computing $T$. Each of these blue
        triangles are set using our $\tau$ vector}\label{fig:termCase}.
    \end{figure}
    \subsection{Matrix Operation LARFT}
    \begin{theorem}\label{thm:Puglisi}
        Theorem 2 in \cite{Joff}:

        Let $U\in\R^{m\times k}$ have linearly independent columns. Then, there exists
        a unique non-singular upper triangular matrix $S\in\R^{k\times k}$ such that
        $I-USU^\top$ is an orthogonal matrix. This matrix $S$ satisfies $S=T^{-1}$ with
        $T+T^\top = U^\top U$, where $T\in\R^{k\times k}$ is itself a unique non-singular upper 
        triangular matrix.
    \end{theorem}
    With this theorem coupled with work done in \cite{Puglisi}, we have an algorithm to compute the $T$ that 
    we need. This algorithm is heavily inspired by \cite{Joff} and is given by algorithm \ref{alg:UTDLARFT}

    \begin{algorithm}
        \caption{DLARFT implementation based on \cite{Joff} and \cite{Puglisi}}\label{alg:UTDLARFT}
        \begin{algorithmic}[1]
            \REQUIRE $V\in\R^{m\times n}, T\in\R^{n\times n}, \tau\in\R^n$\hfill\COMMENT{$m\geq n\geq 1$}
            \STATE $T = \text{upperTriangularPart}\left(V^\top V\right)$
            \FOR{ $i=1,\cdots n$ }
                \STATE $T(i,i) = \frac{T(i,i)}{2}$
            \ENDFOR
            \STATE $T=T^{-1}$
        \end{algorithmic}
    \end{algorithm}

    \subsection{Numerical Experiments}
    We ran numerical experiments on the \href{https://ccm-docs.readthedocs.io/en/latest/alderaan/#hardware}{cluster}
    here at CU Denver. We can see the performance in Figure~\ref{fig:DLARFT}. The key takeaway is that our new
    scheme achieves roughly 3 times more performance than existing algorithms inside LAPACK as well as AOCL.
    \begin{figure}
        \centering
        \includegraphics[width=.45\textwidth]{figures/timeDLARFT.pdf}
        \includegraphics[width=.45\textwidth]{figures/flopDLARFT.pdf}
        \caption{Comparison of varying DLARFT versions with fixed $n$.}\label{fig:DLARFT}
    \end{figure}
    \subsection{Computational Cost}
    By looking at our implementation along with reference LAPACK, we get the following operation counts that are
    used in the previous figures.
    \[
    \begin{aligned}
            \text{DLARFT: }&\,      \frac{6mk^2 - 6mk -4k^3 +6k^2 - 2k}{6}&\approx mk^2 - \frac{2k^3}{3}\\
            \text{DLARFT\_REC: }&\, \frac{6mk^2 - 6mk -2k^3 +3k^2 -  k}{6}&\approx mk^2 - \frac{k^3}{3}\\
            \text{DLARFT\_MAT: }&\, \frac{6mk^2 + 6mk +4k^3 -9k^2 + 5k}{6}&\approx mk^2 + \frac{2k^3}{3}
    \end{aligned}
    \]
    \section{DLARFB}
    In order to apply the block of reflectors to our already formed columns of $Q$, we use \verb|DLARFB|.
    However the existing algorithm is a general one, which we will present first then discuss 
    specialization to our algorithm.
    \subsection{Existing behavior}
        Given $V\in\R^{m\times k}$, $T\in\R^{k\times k}$, and $C\in\R^{m\times n}$, we 
        want to compute $C = HC$ where $H$ has the form
        $$
        H = I - VTV^\top
        $$
        $V$ is unit lower triangular, $T$ is upper triangular, and $C$ is any matrix of the proper shape. 

        We also break up $V$ and $C$ as follows
        \begin{align}
            V &= \begin{bmatrix} V_1 \\ V_2 \end{bmatrix} \label{eq:vMatVert} \\
            C &= \begin{bmatrix} C_1 \\ C_2 \end{bmatrix} \label{eq:cMatVert}
        \end{align}
        Where $V_1\in\R^{k\times k}$ is unit lower triangular, and $C_1\in\R^{k\times n}$.

        By substituting in this breaking down of $C$ and $V$ into our equation $C = HC$, we can write each of the
        components as follows
        \begin{align*}
            C_1 &= C_1 - V_1T\left(V_1^\top C_1 + V_2^\top C_2\right) \\
            C_2 &= C_2 - V_2T\left(V_1^\top C_1 + V_2^\top C_2\right).
        \end{align*}
        Looking at this characterization, we can see that if we can exploit some fact of $C$ then we 
        can potentially help out our main driver.

        The current implementation is given by Algorithm~\ref{alg:refDLARFB}
        \begin{algorithm}
            \caption{Reference DLARFB}\label{alg:refDLARFB}
            \begin{algorithmic}[1]
                \REQUIRE $V\in\R^{m\times k}, T\in\R^{k\times k}, C\in\R^{m\times n}$
                \STATE $W = $ new matrix in $\R^{k\times k}$
                \STATE $W = C_1^\top$
                \STATE $W = WV_1$
                \STATE $W = W + C_2^\top V_2$
                \STATE $W = WT^\top$
                \STATE $C_2 = C_2 - V_2W^\top$
                \STATE $W = WV_1^\top$
                \STATE $C_1 = C_1 - W^\top$
            \end{algorithmic}
        \end{algorithm}
        
        However, for us, we get that $C_1$ is identically $0$, so let us try to exploit it, which we do in 
        Algorithm~\ref{alg:DLARFB0C2}
    \subsection{New Behavior}
    Since we get from Algorithm~\ref{alg:dorgqr} that $C_1 = 0$, it is of the same size as $W$ from 
    Algorithm~\ref{alg:refDLARFB}, and we will be overwriting it anyway, we can actually use this 
    region as a workspace, which gives us Algorithm~\ref{alg:DLARFB0C2}.
    \begin{algorithm}
        \caption{DLARFB\_0C2}\label{alg:DLARFB0C2}
        \begin{algorithmic}[1]
            \REQUIRE $V\in\R^{m\times k}, T\in\R^{k\times k}, C\in\R^{m\times n}$
            \STATE $C_1 = V_2^\top C_2$
            \STATE $C_1 = TC_1$
            \STATE $C_2 = C_2 - V_2C_1$
            \STATE $C_1 =     - V_1C_1$
        \end{algorithmic}
    \end{algorithm}
    This saves a call to \verb|DTRMM| and \verb|DGEMM|.
    \subsection{Flop Comparison}
        For these two different versions, we get the following computation cost for $V\in\R^{m\times k}$, $T\in\R^{k\times k}$, and $C\in\R^{m\times n}$.
%\footnote{
Note: In our actual loops, we grow $m$ and $n$, and $k$ is the blocking parameter usually just set to $32$.%}
        \begin{align*}
            \text{DLARFB: }&\, 4mnk - nk^2 + nk\\
            \text{DLARFB\_0C2: }&\, 4mnk - 2nk^2
        \end{align*}
        In addition to our memory savings, we also have a slight computation improvement. We do not see
        much of this benefit in computational cost in practice though. Note: we are also linear in $m$ and
        $n$, so we scale well when we treat $k$ as a small blocking parameter
    \subsection{Numerical Experiments}
        \begin{figure}
            \centering
            \includegraphics[width=.45\textwidth]{figures/timeDLARFB.pdf}
            \includegraphics[width=.45\textwidth]{figures/flopDLARFB.pdf}
            \caption{Comparison of our specialized DLARFB to existing DLARFB calls}\label{fig:DLARFB}
        \end{figure}

        In Figure~\ref{fig:DLARFB}, we see that while we do not have much of a performance increase,
        we see the advantage in no longer needing a workspace, which is a benefit we will see the 
        consequences of more so in Figure~\ref{fig:DORGKR} where we can take advantage of investigating
        increasing the block sizes.
    \section{Main Driver Algorithm}
    \subsection{Saving Flops on first iteration}
    If we refer back to Algorithm~\ref{alg:refDLARFB} and Algorithm~\ref{alg:DLARFB0C2}, we see that if we can also
    use the fact that $C_2=I$, we can save even more operations! Algorithm~\ref{alg:DLARFBInit} shows what we 
    need to do in this case.
    
    \begin{algorithm}
        \caption{DLARFB\_0I}\label{alg:DLARFBInit}
        \begin{algorithmic}[1]
            \STATE $C_1 = TV_2^\top$
            \STATE $C_2 = C_2 - V_2C_1$
            \STATE $C_1 =     - V_1C_1$
        \end{algorithmic}
    \end{algorithm}
    
    In comparison to Algorithm~\ref{alg:DLARFB0C2}, we save a call to \verb|DGEMM|, which is the most expensive 
    call inside \verb|DLARFB|. To be more precise, if we have a block of size $\nb$ then we save
    \[
        2\nb\left(m-k\right)\left(n-k\right)
    \]
    In our case, this is usually $32$, so we save
    \[
        64\left(m-k\right)\left(n-k\right)
    \]
    operations. 

    In the case of $m=n\geq k$ we have the opportunity for this specialization to lead to some interesting
    performance improvements.
    \section{DORG2R}
    When we employ Algorithm~\ref{alg:dorg2r}, we are usually doing so via calling the routine \verb|DORG2R|
    For our case, we are using this routine in order to compute the columns of the block we are currently 
    working on. However, we end up needing to recompute some of our information inside of $T$. So, if we want to
    instead use this information we already have, we get our algorithm discussed in the next section.
    \section{DORGKR}
    \subsection{Algorithm Overview}
    We want to compute the matrix $Q_n$ given by equation \ref{eq:Qn}. Notice that this is slightly different than
    before where we wanted to have our theory apply to the full $Q$. Since we only want to compute our current
    panel, we don't want to consider any columns passed $n$ at all. Our algorithm is given by 
    Algorithm~\ref{alg:dorgkr}. We will be assuming that we no longer need to keep $T$ as we call this routine
    at the end of our loop in Algorithm~\ref{alg:dorgqr}, so it will be recomputed when the loop is restarted.

    In addition, we will need $V\in\R^{m\times n}, T\in\R^{n\times n}$. So we break up $V$ as in equation 
    \ref{eq:vMatVert}, and $Q$ similarly to Equation~\ref{eq:cMatVert}.
    \begin{algorithm}
        \caption{DORGKR}\label{alg:dorgkr}
        \begin{algorithmic}[1]
            \REQUIRE{ $V\in\R^{m\times n}, T\in\R^{n\times n}$ }
            \STATE $T = TV_1^\top$
            \STATE $Q_2 = -V_2T$
            \STATE $Q_1 = -V_1T$
            \STATE $Q = I - Q$
        \end{algorithmic}
    \end{algorithm}
    \subsection{Numerical Experiments}
    We want to compare the performance of \verb|DORGKR| to \verb|DORG2R|. To do so, we restrict to the case of 
    $n=k$ and vary this choice for a fixed $m$, which is displayed in figure \ref{fig:DORGKR}.

    \begin{figure}
        \centering
        \includegraphics[width=.45\textwidth]{figures/timeDORGKR.pdf}
        \includegraphics[width=.45\textwidth]{figures/flopDORGKR.pdf}
        \caption{Comparison of DORGKR against DORG2R for a fixed value of $m$ and varying $n=k$.}\label{fig:DORGKR}
    \end{figure}

    We see some great improvements for cases of large $n$ which would correspond to a larger block size parameter.
    So this new behavior should be investigated further.
    \subsection{Computational Cost}
    When we compute the operation cost of our new algorithm versus \verb|DORG2R| gives us the following
    operation counts.
        Assuming we have $V\in\R^{m\times n}$ and $T\in\R^{n\times n}$ our costs are 
        $$
        \begin{aligned}
            % Note: This is only true for the k = n case for DORG2R
            \text{DORG2R: }&\, \dfrac{12mn^2 - 6mn -4n^3 +9n^2 - 11n}{6} &\approx 2mn^2 - \frac{2n^3}{3}\\
            \text{DORGKR: }&\, \dfrac{2mn^2 + n^2 - n}{2} &\approx mn^2
        \end{aligned}
        $$
        We see an improvement for a factor of $2$, which is non-negligible, which is due to the fact that we are exploiting information from $T$.

        We should note that the sizes used within our routine are relatively small, so we will not see 
        this improvement much when we put it all together
    \section{Putting it all together}
    Putting all of our new pieces together leaves us with a modified version of Algorithm~\ref{alg:dorgqr}, which
    is given by Algorithm~\ref{alg:myDorgqr}.
    \begin{algorithm}
        \caption{New DORGQR}\label{alg:myDorgqr}
        \begin{algorithmic}[1]
            \STATE Determine blocking parameter $\nb$
            \STATE $Q = I_{m\times n}$
            \STATE Compute the columns of $Q$ with \verb|DLARFB_0I|
            \FOR{Each block of size $\nb$ of $V$ moving right to left}
                \STATE (algorithm \ref{alg:UTDLARFT}) Compute $T$
                \STATE (\verb|DLARFB_0C2|) Apply $I-V_{\nb}TV_{\nb}^\top$ to the trailing columns of $Q$
                \STATE (\verb|DORGKR|) Apply $I-V_{\nb}TV_{\nb}^\top$ to itself
            \ENDFOR
        \end{algorithmic}
    \end{algorithm}

    \subsection{Numerical Experiments}
    We can also see our performance in comparison to the existing \verb|DORGQR| implementation in figures 
    \ref{fig:dorgqr} and \ref{fig:dorgqrSquare}, which are for the $m\geq n=k$ and $m=n\geq k$ cases respectively.
    As we note in caption of figure \ref{fig:dorgqrSquare}, we observe some strange behavior that could warrant
    some further investigation.
    \begin{figure}
        \centering
            \includegraphics[width=.45\textwidth]{figures/timeDORGQR.pdf}
            \includegraphics[width=.45\textwidth]{figures/flopDORGQR.pdf}
        \caption{Comparison of Algorithm~\ref{alg:dorgqr} and Algorithm~\ref{alg:myDorgqr} when $m\geq n=k$}\label{fig:dorgqr}
    \end{figure}
    \begin{figure}
        \centering
            \includegraphics[width=.45\textwidth]{figures/timeDORGQRSquare.pdf}
            \includegraphics[width=.45\textwidth]{figures/flopDORGQRSquare.pdf}
        \caption{Comparison of algorithms \ref{alg:dorgqr} and \ref{alg:myDorgqr} when $m=n\geq k$. 
        We see some odd behavior here especially in regards to the timing graph as we do not see an increase 
        in execution time for larger problems, which would be expected}\label{fig:dorgqrSquare}
    \end{figure}
    \section{Source Code}
    The code for this software can be found in the repository \href{https://github.com/jprhyne/Fall23}{Fall23}
    \section{References}
    \bibliographystyle{plain}
    \bibliography{report}

\end{document}
